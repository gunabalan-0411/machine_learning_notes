{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most ML engineers just use common loss functions like RMSE\n",
    "or MAE (mean absolute error) for regression, logistic loss (also log loss) for binary\n",
    "classification, and cross entropy for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalty for Wrong Predictions (Nonlinear Scaling):\n",
    "\n",
    "The log function grows very negative as the predicted probability approaches zero.\n",
    "This means incorrect predictions are penalized much more heavily if the model is very confident in the wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1086626245216111"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cross_entropy(p, q):\n",
    "    return -sum([p[i] * np.log(q[i]) for i in range(len(p))])\n",
    "p = [0, 0, 0, 1]\n",
    "q = [0.45, 0.2, 0.02, 0.33]\n",
    "cross_entropy(p, q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
